#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test spÃ©cifique pour corriger le problÃ¨me Hugging Face
"""

import requests
import sys

def test_hf_models(token):
    """Test tous les modÃ¨les HF disponibles"""
    
    models = [
        {
            'name': 'microsoft/DialoGPT-small',
            'url': 'https://api-inference.huggingface.co/models/microsoft/DialoGPT-small',
            'description': 'DialoGPT Small - Conversationnel lÃ©ger'
        },
        {
            'name': 'microsoft/DialoGPT-medium',
            'url': 'https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium',
            'description': 'DialoGPT Medium - Conversationnel'
        },
        {
            'name': 'facebook/blenderbot-400M-distill',
            'url': 'https://api-inference.huggingface.co/models/facebook/blenderbot-400M-distill',
            'description': 'BlenderBot - Conversationnel distillÃ©'
        },
        {
            'name': 'gpt2',
            'url': 'https://api-inference.huggingface.co/models/gpt2',
            'description': 'GPT-2 - GÃ©nÃ©ration de texte classique'
        },
        {
            'name': 'distilgpt2',
            'url': 'https://api-inference.huggingface.co/models/distilgpt2',
            'description': 'DistilGPT-2 - Version lÃ©gÃ¨re de GPT-2'
        },
        {
            'name': 'google/flan-t5-small',
            'url': 'https://api-inference.huggingface.co/models/google/flan-t5-small',
            'description': 'FLAN-T5 Small - Question-rÃ©ponse'
        }
    ]
    
    headers = {"Authorization": f"Bearer {token}"}
    working_models = []
    
    print("ğŸ§ª Test des modÃ¨les Hugging Face disponibles...")
    print("=" * 60)
    
    for model in models:
        print(f"\nğŸ“Š Test: {model['name']}")
        print(f"   ğŸ“ {model['description']}")
        
        try:
            # Adapter le payload selon le modÃ¨le
            if 'flan-t5' in model['name'].lower():
                payload = {
                    "inputs": "Question: Comment allez-vous? RÃ©ponse:",
                    "parameters": {"max_new_tokens": 30}
                }
            elif 'blenderbot' in model['name'].lower():
                payload = {
                    "inputs": "Hello",
                    "parameters": {"max_new_tokens": 30}
                }
            else:  # GPT-2, DialoGPT
                payload = {
                    "inputs": "Hello, how are you?",
                    "parameters": {"max_new_tokens": 30, "temperature": 0.7}
                }
            
            response = requests.post(model['url'], headers=headers, json=payload, timeout=20)
            
            if response.status_code == 200:
                result = response.json()
                
                generated_text = ""
                if isinstance(result, list) and len(result) > 0:
                    generated_text = result[0].get('generated_text', '')
                elif isinstance(result, dict):
                    generated_text = result.get('generated_text', '')
                
                if generated_text and len(generated_text.strip()) > 5:
                    print(f"   âœ… FONCTIONNEL")
                    print(f"   ğŸ“¤ RÃ©ponse: {generated_text[:80]}...")
                    working_models.append(model)
                else:
                    print(f"   âŒ RÃ©ponse vide ou trop courte")
                    
            elif response.status_code == 503:
                print(f"   â³ ModÃ¨le en cours de chargement")
            elif response.status_code == 403:
                print(f"   ğŸ”’ Permissions insuffisantes")
            else:
                print(f"   âŒ Erreur HTTP {response.status_code}")
                print(f"      {response.text[:100]}...")
                
        except Exception as e:
            print(f"   âŒ Exception: {str(e)}")
    
    print("\n" + "=" * 60)
    print(f"ğŸ“ˆ RÃ‰SULTATS: {len(working_models)}/{len(models)} modÃ¨les fonctionnels")
    
    if working_models:
        print("\nâœ… ModÃ¨les disponibles avec votre token:")
        for model in working_models:
            print(f"   â€¢ {model['name']} - {model['description']}")
        return working_models[0]['name']  # Retourner le premier qui fonctionne
    else:
        print("\nâŒ Aucun modÃ¨le accessible avec ce token")
        print("\nğŸ’¡ Solutions possibles:")
        print("   1. VÃ©rifiez que votre token HF est correct")
        print("   2. CrÃ©ez un nouveau token avec les permissions 'Inference API'")
        print("   3. Attendez quelques minutes (modÃ¨les en cours de chargement)")
        print("   4. Utilisez OpenAI ou Anthropic Ã  la place")
        return None

def main():
    print("ğŸ¤— TEST DE CORRECTION HUGGING FACE")
    print("=" * 50)
    
    if len(sys.argv) > 1:
        token = sys.argv[1]
    else:
        token = input("\nğŸ”‘ Entrez votre token Hugging Face: ").strip()
    
    if not token:
        print("âŒ Token requis")
        return
    
    working_model = test_hf_models(token)
    
    if working_model:
        print(f"\nğŸ‰ SOLUTION TROUVÃ‰E!")
        print(f"   ModÃ¨le recommandÃ©: {working_model}")
        print(f"   WaveAI utilisera automatiquement ce modÃ¨le.")
    else:
        print(f"\nâš ï¸  Aucun modÃ¨le HF accessible")
        print(f"   Recommandation: Utilisez OpenAI ou Anthropic")

if __name__ == "__main__":
    main()